{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "data_augmentation.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXbwlcZb4Xj5"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpG4R-bD48Ml"
      },
      "source": [
        "import csv\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.corpus import stopwords, subjectivity\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from scipy.sparse import csr_matrix, hstack, vstack\n",
        "from scipy.sparse.linalg import svds\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.svm import SVC\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def tockenize_(text):\n",
        "    if type(text) == list:\n",
        "        text = ' '.join(text)\n",
        "    text = re.sub(r\" (n't|'[a-z]{1,3})\", r'\\1', text)\n",
        "    text = re.sub(r'[^a-z0-9\\s\\'\\-\\.\\?!,\\\"]', '', text.lower())\n",
        "    text = ' '.join([lemmatizer.lemmatize(w) for w in tokenizer.tokenize(text)])\n",
        "    return text\n",
        "\n",
        "\n",
        "def vader_sentinet(word):\n",
        "    vader_scores = sid.polarity_scores(word)\n",
        "    v_neg = vader_scores['neg']\n",
        "    v_neu = vader_scores['neu']\n",
        "    v_pos = vader_scores['pos']\n",
        "    \n",
        "    senti_net = list(swn.senti_synsets(word))\n",
        "    if len(senti_net) > 0:\n",
        "        s_neg = senti_net[0].neg_score()\n",
        "        s_pos = senti_net[0].pos_score()\n",
        "        s_obj = senti_net[0].obj_score()\n",
        "    else:\n",
        "        s_neg, s_pos, s_obj = 0, 0, 0\n",
        "    return v_neg, v_neu, v_pos, s_neg, s_pos, s_obj\n",
        "\n",
        "\n",
        "def mean_senti_vader_score(text):\n",
        "    if type(text) == str:\n",
        "        text = text.split()\n",
        "    score_lists = [[] for _ in range(6)]\n",
        "    for w in text:\n",
        "        scores = vader_sentinet(w)\n",
        "        for i, s in enumerate(scores):\n",
        "            score_lists[i].append(s)\n",
        "    mean_scores = [sum(l) / max(1, len(l)) for l in score_lists]\n",
        "    return mean_scores\n",
        "\n",
        "\n",
        "def nb_senti_words(text):\n",
        "    if type(text) == str:\n",
        "        text = text.split()\n",
        "    d = {-1 : 0, 0 : 0, 1 : 0, 2 : 0, 3 : 0, 4 : 0}\n",
        "    for w in text:\n",
        "        if w in oneword_sentiment:\n",
        "            d[oneword_sentiment[w]] += 1\n",
        "    for k in d:\n",
        "        d[k] /= max(1, len(text))\n",
        "        \n",
        "    return d[0], d[1], d[2], d[3], d[4], d[-1]\n",
        "\n",
        "\n",
        "def vader_score(text):\n",
        "    scores = sid.polarity_scores(text)\n",
        "    return scores['neg'], scores['neu'], scores['pos'], scores['compound']\n",
        "\n",
        "\n",
        "def undersample_Xy(X, y):\n",
        "    y_argmax = y.argmax(axis = 1)\n",
        "    y_counts = Counter(y_argmax)\n",
        "    nb_to_sample = sorted(y_counts.values())[1]\n",
        "    \n",
        "    def undr(label):\n",
        "        X_by_label = X[y_argmax == label]\n",
        "        y_by_label = y[y_argmax == label]\n",
        "        count_for_label = X_by_label.shape[0]\n",
        "        sampled_indices = np.random.choice(np.arange(count_for_label), \n",
        "                                   size = min(nb_to_sample, count_for_label), replace = False)\n",
        "        undersampled_X = X_by_label[sampled_indices]\n",
        "        undersampled_y = y_by_label[sampled_indices]\n",
        "        return undersampled_X, undersampled_y\n",
        "    \n",
        "    Xs, ys = zip(*[undr(label) for label in y_counts.keys()])\n",
        "    return vstack(Xs), vstack(ys)\n",
        "\n",
        "\n",
        "data = pd.read_csv('train.tsv.zip', sep=\"\\t\")\n",
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBwDbYhz5Xxr"
      },
      "source": [
        "data = data.rename(columns = {'Phrase' : 'Text', 'Sentiment' : 'Score'})\n",
        "data_shuffle_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
        "for i, (train_indices, test_indices) in enumerate(data_shuffle_split.split(data, data.Score)):\n",
        "    data = data.iloc[train_indices].reset_index(drop = True)\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14BUUH3tB3OX"
      },
      "source": [
        "tokenizer = RegexpTokenizer(r\"['\\-\\w\\.\\?!,]+\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stops = stopwords.words('english')\n",
        "tqdm.pandas()\n",
        "data['Tokens'] = data.Text.apply(tockenize_)\n",
        "data['nb_words'] = data.Tokens.str.count(' ') + 1\n",
        "data['rel_nb_words'] = data.nb_words / data.nb_words.max()\n",
        "data['nb_chars'] = data.Tokens.str.len()\n",
        "data['rel_nb_chars'] = data.nb_chars / data.nb_chars.max()\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POgSjVAMvfiV"
      },
      "source": [
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "oneword_sentiment = {}\n",
        "df_len1 = data[data.nb_words == 1]\n",
        "for label in df_len1.Score.value_counts().to_dict().keys():\n",
        "    for word in df_len1.Tokens[df_len1.Score == label]:\n",
        "        oneword_sentiment[word] = label\n",
        "\n",
        "data = data.merge(data.Tokens.apply(lambda t: pd.Series(nb_senti_words(t))), \n",
        "                left_index=True, right_index=True)\n",
        "data = data.rename(columns={\n",
        "                        0 : \"nb_senti_0\", \n",
        "                        1 : \"nb_senti_1\", \n",
        "                        2 : \"nb_senti_2\", \n",
        "                        3 : \"nb_senti_3\",\n",
        "                        4 : \"nb_senti_4\",\n",
        "                        5 : \"nb_senti_-1\"\n",
        "                       })\n",
        "\n",
        "print('\\tvader/sentinet')\n",
        "data = data.merge(data.Tokens.apply(lambda t: pd.Series(mean_senti_vader_score(t))), \n",
        "                left_index=True, right_index=True)\n",
        "data = data.rename(columns={\n",
        "                        0 : \"v_neg\", \n",
        "                        1 : \"v_neu\", \n",
        "                        2 : \"v_pos\", \n",
        "                        3 : \"s_neg\",\n",
        "                        4 : \"s_pos\",\n",
        "                        5 : \"s_obj\"\n",
        "                       })\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGTfu1vo1K4m"
      },
      "source": [
        "subj_docs = []\n",
        "for l in tqdm(['subj', 'obj']):\n",
        "    for s in subjectivity.sents(categories=l):\n",
        "        subj_docs.append((l, tockenize_(s)))\n",
        "\n",
        "review_docs = []\n",
        "for l in tqdm(['pos', 'neg']):\n",
        "    for s in movie_reviews.sents(categories=l):\n",
        "        review_docs.append((l, tockenize_(s)))\n",
        "\n",
        "subj_df = pd.DataFrame(subj_docs, columns = ['label', 'text'])\n",
        "review_df = pd.DataFrame(review_docs, columns = ['label', 'text'])\n",
        "\n",
        "subj_cv = CountVectorizer(binary = True, min_df = 5, max_df = .5, dtype = np.int8).fit(subj_df.text)\n",
        "review_cv = CountVectorizer(binary = True, min_df = 5, max_df = .5, dtype = np.int8).fit(review_df.text)\n",
        "\n",
        "X_subj = subj_cv.transform(subj_df.text)\n",
        "X_review = review_cv.transform(review_df.text)\n",
        "\n",
        "subj_logit = LogisticRegressionCV(n_jobs = 3, max_iter = 400, \n",
        "                                  cv=6, random_state=0, verbose = True).fit(X_subj, subj_df.label)\n",
        "review_logit = LogisticRegressionCV(n_jobs = 3, max_iter = 400, \n",
        "                                  cv=6, random_state=0, verbose = True).fit(X_review, review_df.label)\n",
        "\n",
        "print('subj', subj_logit.score(X_subj, subj_df.label))\n",
        "print('polarity', review_logit.score(X_review, review_df.label))\n",
        "\n",
        "data['subj_0'], data['subj_1'] = zip(*subj_logit.predict_proba(subj_cv.transform(data.Tokens)))\n",
        "data['review_0'], data['review_1'] = zip(*review_logit.predict_proba(review_cv.transform(data.Tokens)))\n",
        "\n",
        "del subj_df, subj_logit, subj_cv\n",
        "del review_df, review_logit, review_cv\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNUFxMGG7HUh"
      },
      "source": [
        "data = data.merge(data.Tokens.apply(lambda t: pd.Series(vader_score(t))), \n",
        "         left_index=True, right_index=True)\n",
        "data = data.rename(columns={0 : \"vader_neg\", 1 : \"vader_neu\", 2 : \"vader_pos\", 3 : \"vader_compound\"})\n",
        "cv = CountVectorizer(binary = True, max_df = .5, min_df = 5, \n",
        "                        dtype = np.int8).fit(data.Tokens)\n",
        "X_text = cv.transform(data.Tokens) #.toarray()\n",
        "print('starting shape:', X_text.shape)\n",
        "\n",
        "tockenizer_app = Tokenizer()\n",
        "tockenizer_app.fit_on_texts(data.Tokens)\n",
        "\n",
        "X_seq_list = tockenizer_app.texts_to_sequences(data.Tokens)\n",
        "seq_len = max(max(X_seq_list, key=len))\n",
        "X_seq = pad_sequences(X_seq_list, maxlen=seq_len)\n",
        "\n",
        "X_other = np.array(data[['vader_neg', 'vader_neu', 'vader_pos', 'vader_compound', \n",
        "                       'v_neg', 'v_neu', 'v_pos', 's_neg', 's_pos', 's_obj',\n",
        "                       'nb_words', 'nb_chars', 'rel_nb_words', 'rel_nb_chars',\n",
        "                       'nb_senti_-1', 'nb_senti_0', 'nb_senti_1', 'nb_senti_2', 'nb_senti_3', 'nb_senti_4', \n",
        "                       'subj_0', 'subj_1', 'review_0']])\n",
        "X = hstack((X_text, csr_matrix(X_other))).tocsr()\n",
        "\n",
        "\n",
        "oh = OneHotEncoder().fit(data.Score.to_numpy().reshape(-1, 1))\n",
        "y = oh.transform(data.Score.to_numpy().reshape(-1,1)).toarray()\n",
        "\n",
        "print(X.shape, X_seq.shape)\n",
        "print(y.shape)\n",
        "del X_seq_list\n",
        "# undersample_Xy(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm993XH63SRF"
      },
      "source": [
        "# Word Embeddings Augmente\n",
        "\n",
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "text = 'The quick brown fox jumps over the lazy dog .'\n",
        "# Substitute word by spelling mistake words dictionary\n",
        "aug = naw.SpellingAug()\n",
        "augmented_texts = aug.augment(text, n=3)\n",
        "\n",
        "# Insert word randomly by word embeddings similarity\n",
        "# model_type: word2vec, glove or fasttext\n",
        "aug = naw.WordEmbsAug(\n",
        "    model_type='word2vec', model_path=model_dir+'GoogleNews-vectors-negative300.bin',\n",
        "    action=\"insert\")\n",
        "augmented_text = aug.augment(text)\n",
        "\n",
        "# Substitute word by word2vec similarity\n",
        "\n",
        "# model_type: word2vec, glove or fasttext\n",
        "aug = naw.WordEmbsAug(\n",
        "    model_type='word2vec', model_path=model_dir+'GoogleNews-vectors-negative300.bin',\n",
        "    action=\"substitute\")\n",
        "augmented_text = aug.augment(text)\n",
        "\n",
        "# Substitute word by TF-IDF similarity\n",
        "\n",
        "aug = naw.TfIdfAug(\n",
        "    model_path=os.environ.get(\"MODEL_DIR\"),\n",
        "    action=\"substitute\")\n",
        "augmented_text = aug.augment(text)\n",
        "\n",
        "# Insert word by contextual word embeddings (BERT, DistilBERT, RoBERTA or XLNet)\n",
        "\n",
        "aug = naw.ContextualWordEmbsAug(\n",
        "    model_path='bert-base-uncased', action=\"insert\")\n",
        "augmented_text = aug.augment(text)\n",
        "\n",
        "# Substitute word by contextual word embeddings (BERT, DistilBERT, RoBERTA or XLNet)\n",
        "aug = naw.ContextualWordEmbsAug(\n",
        "    model_path='bert-base-uncased', action=\"substitute\")\n",
        "augmented_text = aug.augment(text)\n",
        "\n",
        "# Substitute word by WordNet's synonym\n",
        "aug = naw.SynonymAug(aug_src='wordnet')\n",
        "augmented_text = aug.augment(text)\n",
        "\n",
        "# Substitute word by antonym\n",
        "aug = naw.AntonymAug()\n",
        "_text = 'Good boy'\n",
        "augmented_text = aug.augment(_text)\n",
        "\n",
        "# Back Translation Augmenter\n",
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "# text = 'The quick brown fox jumped over the lazy dog'\n",
        "back_translation_aug = naw.BackTranslationAug(\n",
        "    from_model_name='transformer.wmt19.en-de', \n",
        "    to_model_name='transformer.wmt19.de-en'\n",
        ")\n",
        "back_translation_aug.augment(text)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}