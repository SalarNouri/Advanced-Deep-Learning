{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMZXU7tFL1a3"
      },
      "source": [
        "!wget \"http://datasets.d2.mpi-inf.mpg.de/mateusz14visual-turing/nyu_depth_images.tar\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miWSq6RGU_Lf"
      },
      "source": [
        "!wget \"https://datasets.d2.mpi-inf.mpg.de/mateusz14visual-turing/qa.894.raw.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etuNTRLwOQhQ"
      },
      "source": [
        "!wget \"http://nlp.stanford.edu/data/glove.6B.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgKZCW2vPwJn"
      },
      "source": [
        "!unzip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKzRECEoNkyY"
      },
      "source": [
        "!tar -xvf nyu_depth_images.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MINOXoygJnhT"
      },
      "source": [
        "import copy\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.cuda as cuda\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as torchdata\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "BATCH_SIZE: int = 32  \n",
        "EPOCHES: int = 20\n",
        "TEST_LENGTH: int = 2500\n",
        "\n",
        "glove_path = '/content/glove.6B.300d.txt'\n",
        "images_path = '/content/nyu_depth_images/'\n",
        "\n",
        "def load_glove(path):\n",
        "  with open(path) as f:\n",
        "    glove = {}\n",
        "    for line in f.readlines():\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      vector = np.array(values[1:], dtype='float32')\n",
        "      glove[word] = vector\n",
        "    return glove\n",
        "\n",
        "\n",
        "def create_embedding_layer(weights_matrix, non_trainable=False):\n",
        "  num_embeddings, embedding_dim = weights_matrix.shape\n",
        "  embedding_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "  embedding_layer.load_state_dict({'weight': torch.from_numpy(weights_matrix)})\n",
        "  if non_trainable:\n",
        "    embedding_layer.weight.requires_grad = False\n",
        "  return embedding_layer, num_embeddings, embedding_dim\n",
        "\n",
        "\n",
        "class DAQUAR_Dataset(Dataset):\n",
        "  def __init__(self, questions, answers, file_path, transform= None):\n",
        "    self.path = file_path\n",
        "    self.transforms = transform\n",
        "    self.questions = questions\n",
        "    self.answers = answers\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.answers)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    current_question = self.questions[item]\n",
        "    current_answer = self.answers[item]\n",
        "    \n",
        "    for i in range(len(current_question)):\n",
        "      if 'image' in current_question[i]:\n",
        "        index = copy.deepcopy(current_question[i])\n",
        "        if index.replace('image','') == '':\n",
        "          index = 'image1'\n",
        "        current_question[i] = 'image'\n",
        "    img_path = self.path + index +'.png'\n",
        "    img = Image.open(img_path)\n",
        "    if self.transforms is not None:\n",
        "      img = self.transforms(img)\n",
        "    \n",
        "    designed_answer = []\n",
        "    designed_answer.append(current_answer[0])\n",
        "    sample = random.sample(current_question, 5)\n",
        "    designed_answer.extend(sample)\n",
        "    sample = random.sample(vocabs, 26)\n",
        "    designed_answer.extend(sample)\n",
        "    \n",
        "    answer_index = [final_vocab[x] for x in designed_answer]\n",
        "    question_index = [final_vocab[x] for x in current_question]\n",
        "    length_idx = len(question_index)\n",
        "    while (len(question_index) < 31):\n",
        "      question_index.append(0)\n",
        "    \n",
        "    return img, torch.tensor(question_index), torch.tensor(answer_index), torch.tensor(length_idx)\n",
        "    \n",
        "\n",
        "class network(nn.Module):\n",
        "  def __init__(self, matrix_weights):\n",
        "    super(network, self).__init__()\n",
        "    self.resnet = models.resnet18(pretrained = True)\n",
        "    self.resnet.fc = nn.Sequential()\n",
        "    self.linear_from_resnet_to_lstm = nn.Linear(512, 150)\n",
        "    self.embedding, num_embeddings, embedding_dim = create_embedding_layer(weights_matrix, True)\n",
        "    self.gru = nn.GRU(embedding_dim, 150, 1, batch_first=True)\n",
        "    self.fc1 = nn.Linear(662, 300)\n",
        "    self.fc2 = nn.Linear(300, 300)\n",
        "  \n",
        "  def forward(self, image, question, answers, lengths):\n",
        "    with torch.no_grad():\n",
        "      image_features1 = self.resnet(image)\n",
        "    image_features = self.linear_from_resnet_to_lstm(image_features1)\n",
        "    image_features = image_features.unsqueeze(0)\n",
        "\n",
        "    embedded = self.embedding(question)\n",
        "    embedded = torch.nn.utils.rnn.pack_padded_sequence(embedded, batch_first=True, lengths=lengths, enforce_sorted=False)\n",
        "\n",
        "    lstm_output = self.gru(embedded, image_features)\n",
        "    concated = torch.cat((lstm_output[1].squeeze(0), image_features1), 1)  \n",
        "\n",
        "    output = self.fc1(concated)\n",
        "    output = F.relu(output)\n",
        "    output = self.fc2(output)\n",
        "\n",
        "    answer_vec = self.embedding(answers)\n",
        "\n",
        "    final_output = torch.einsum('bwf,bf->bw', answer_vec, output)\n",
        "    \n",
        "    return final_output\n",
        "    "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-H25VFrQglZ"
      },
      "source": [
        "# Create word dictionary\n",
        "glove = load_glove(glove_path)\n",
        "# Get vocabs\n",
        "vocabs, all_questions, all_answers = set(), [], []\n",
        "with open('qa.894.raw.txt') as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    vocab = list(line.split())\n",
        "    vocab = [x.replace(',','') for x in vocab]\n",
        "    if any('?' in s for s in vocab):\n",
        "      temp = copy.deepcopy(vocab)\n",
        "      all_questions.append(temp)\n",
        "    else:\n",
        "      temp = copy.deepcopy(vocab)\n",
        "      all_answers.append(temp)\n",
        "    for i in range(len(vocab)):\n",
        "      if 'image' in vocab[i]:\n",
        "        vocab[i] = 'image'\n",
        "    vocabs.update(vocab)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BggnlHtJchTu"
      },
      "source": [
        "matrix_len, weights_matrix = len(vocabs), np.zeros((matrix_len, 300)\n",
        "words_found = 0\n",
        "final_vocab = dict()\n",
        "\n",
        "for i, word in enumerate(vocabs):\n",
        "  try:\n",
        "    if '_' in word:\n",
        "      temp = copy.deepcopy(word)\n",
        "      temp = temp.split('_')\n",
        "      try:\n",
        "        s1 = glove[temp[0]]\n",
        "      except KeyError:\n",
        "        s1 = np.random.normal(scale = 0.6, size=(300, ))\n",
        "      try:\n",
        "        s2 = glove[temp[1]]\n",
        "      except KeyError:\n",
        "        s2 = np.random.normal(scale = 0.6, size=(300, ))\n",
        "      weights_matrix[i] = (s1 + s2) / 2\n",
        "      final_vocab[word] = i\n",
        "      continue\n",
        "    weights_matrix[i] = glove[word]\n",
        "    words_found += 1\n",
        "  except KeyError:\n",
        "    weights_matrix[i] = np.random.normal(scale = 0.6, size=(300, ))\n",
        "  final_vocab[word] = i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzM-xhR2yNPU"
      },
      "source": [
        "transforms_value = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "data = DAQUAR_Dataset(all_questions, all_answers, images_path, transforms_value)\n",
        "data_range = range(len(data))\n",
        "test_index = random.sample(range(len(data)), TEST_LENGTH)\n",
        "train_index = [x for x in data_range if x not in test_index]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_index)\n",
        "test_sampler = SubsetRandomSampler(test_index)\n",
        "\n",
        "train_loader = torchdata.DataLoader(data, batch_size=BATCH_SIZE, shuffle=False, sampler=train_sampler)\n",
        "print(len(train_loader))\n",
        "test_loader = torchdata.DataLoader(data, batch_size=BATCH_SIZE, shuffle=False, sampler=test_sampler)\n",
        "print(len(test_loader))\n",
        "\n",
        "network_model = network(weights_matrix)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(network_model.parameters(), lr=1e-2, weight_decay=5e-3)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "network_model.to(device)\n",
        "optimizer.zero_grad()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VevBE79ahKpB"
      },
      "source": [
        "loss_values, test_loss = list()\n",
        "\n",
        "for epoch in range(EPOCHES):\n",
        "  epoch_loss, epoch_test_loss = list()\n",
        "  optimizer.zero_grad()\n",
        "  my_net.train()\n",
        "  for i, (image, question, answer, length) in enumerate(train_loader):\n",
        "    image = image.to(device)\n",
        "    question = question.to(device)\n",
        "    answer = answer.to(device)\n",
        "    image = Variable(image)\n",
        "\n",
        "    question = Variable(question)\n",
        "    answer = Variable(answer)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = network_model(image, question, answer, length)\n",
        "    label = torch.zeros([len(image)], dtype= torch.long)\n",
        "    loss = criterion(output, label.to(device))\n",
        "    loss.backward()\n",
        "    epoch_loss.append(loss.item())\n",
        "    print('In epoch {} and index {} loss is {}'.format(epoch, i , epoch_loss[-1]))\n",
        "  print('In epoch {} average loss was {}'.format(epoch, np.mean(epoch_loss)))\n",
        "  loss_values.append(np.mean(epoch_loss))\n",
        "  \n",
        "  network_model.eval()\n",
        "  for i, (image, question, answer, length) in enumerate(test_loader):\n",
        "    image = image.to(device)\n",
        "    question = question.to(device)\n",
        "    answer = answer.to(device)\n",
        "\n",
        "    image = Variable(image)\n",
        "    question = Variable(question)\n",
        "    answer = Variable(answer)\n",
        "\n",
        "    output = network_model(image, question, answer, length)\n",
        "    label = torch.zeros([len(image)], dtype= torch.long)\n",
        "    loss = criterion(output, label.to(device))\n",
        "    epoch_test_loss.append(loss.item())\n",
        "    print('In test epoch {} and index {} loss is {}'.format(epoch, i, epoch_test_loss[-1]))\n",
        "  print('In test epoch {} average loss was {}'.format(epoch, np.mean(epoch_test_loss)))\n",
        "  test_loss.append(np.mean(epoch_test_loss))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z47yCn3hgW2"
      },
      "source": [
        "fig = plt.figure(figsize=(10, 10))\n",
        "plt.plot(all_loss,'b', label='train_loss')\n",
        "plt.plot(test_loss,'r', label='test_loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Average loss in epoch')\n",
        "plt.xlabel('# of epoch')\n",
        "plt.ylabel('Average Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}