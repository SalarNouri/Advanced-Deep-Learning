{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitadlvenvbd9a5c10f53f49eca40cc1477bddc462",
   "display_name": "Python 3.8.5 64-bit ('adl': venv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    " # Imbalance classfication problem -- (Code Challenge)\n",
    " The jupyter notebook includes all related algorithms and results to load dataset, visualize principal conponents, and then, prediting the data's label using Logistic regression and a designed neural network."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "NUM_PCA_COMPONENT: int = 2\n",
    "PCA_VARIANCE: np.float = 0.95\n",
    "\n",
    "\n",
    "def get_data() -> List[pd.DataFrame]:\n",
    "    input_train_data = pd.read_csv('train_x.csv')\n",
    "    output_train_data = pd.read_csv('train_y.csv')\n",
    "    input_test_data = pd.read_csv('test_x.csv')\n",
    "    return input_train_data, output_train_data, input_test_data\n",
    "\n",
    "\n",
    "def standardize_data(input_train_data: pd.DataFrame, input_test_data: pd.DataFrame) -> List[np.ndarray]:\n",
    "    in_train = StandardScaler().fit_transform(input_train_data)\n",
    "    in_test = StandardScaler().fit_transform(input_test_data)\n",
    "    return in_train, in_test\n",
    "\n",
    "\n",
    "def project_data(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    pca = PCA(n_components=NUM_PCA_COMPONENT)\n",
    "    principalComponents = pca.fit_transform(data)\n",
    "    principalDf = pd.DataFrame(data = principalComponents,\n",
    "                                columns = ['principal component 1', 'principal component 2'])\n",
    "    return principalDf, pca\n",
    "\n",
    "\n",
    "def visualize_data(final_df: pd.DataFrame):\n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    ax = fig.add_subplot(1,1,1) \n",
    "    ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "    ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "    ax.set_title('2 component PCA', fontsize = 20)\n",
    "    targets = [1, 0]\n",
    "    colors = ['g', 'r']\n",
    "    for target, color in zip(targets,colors):\n",
    "        indicesToKeep = final_df['target'] == target\n",
    "        ax.scatter(final_df.loc[indicesToKeep, 'principal component 1'],\n",
    "                    final_df.loc[indicesToKeep, 'principal component 2'],\n",
    "                    c = color,\n",
    "                    s = 50)\n",
    "    ax.legend(targets)\n",
    "    ax.grid()"
   ]
  },
  {
   "source": [
    "# Visualize Principal Components\n",
    "In this step, the train data and test data will be loaded, then, they shoudl be normalized in order to avoid any problem in not only learning phase but also in computing principal components. Eventually, they data will be shown by considering principal components.\n",
    "\n",
    "Since PCA yields a feature subspace that maximizes the variance along the axes, it makes sense to standardize the data, especially, if it was measured on different scales. Although, all features in the Iris dataset were measured in centimeters, let us continue with the transformation of the data onto unit scale (mean=0 and variance=1), which is a requirement for the optimal performance of many machine learning algorithms.\n",
    "\n",
    "\n",
    "Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual feature do not more or less look like standard normally distributed data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train_data, out_train_data, input_test_data = get_data()\n",
    "in_train, in_test = standardize_data(input_train_data, input_test_data)\n",
    "principal_df, pca = project_data(in_train)\n",
    "final_df = pd.concat([principal_df, out_train_data[['target']]], axis = 1)\n",
    "visualize_data(final_df)\n",
    "print(f\"The pca's variance ratio is {pca.explained_variance_ratio_}\")"
   ]
  },
  {
   "source": [
    "# Applying PCA to the Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(PCA_VARIANCE)\n",
    "pca.fit(in_train)\n",
    "print(f\"Num of principal components is {pca.n_components_}\")\n",
    "train_ = pca.transform(in_train)\n",
    "test_ = pca.transform(in_test)"
   ]
  },
  {
   "source": [
    "# Logistic Regression\n",
    "The fitting problem for train data will be computed by using Logistic Regression in this step. Then, the fitting accuracy will be computed for the seperated test data.\n",
    "\n",
    "accuracy (fraction of correct predictions): correct predictions / total number of data points"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1, test_1, train_lbl, test_lbl = train_test_split(train_, out_train_data.target, test_size=1/8.0, random_state=0)\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "logisticRegr.fit(train_1, train_lbl)\n",
    "score = logisticRegr.score(test_1, test_lbl)\n",
    "print(f\"The prediction score is {score}\")"
   ]
  },
  {
   "source": [
    "# Predcit data\n",
    "The target for the test data will be calculated using the obtained parameters in LogisticRegression. Then, the result is saved as a CSV file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test = logisticRegr.predict(test_)\n",
    "test_y = pd.DataFrame(predicted_test, columns=['target'])\n",
    "test_y['Unnamed: 0'] = out_train_data['Unnamed: 0']\n",
    "test_y = test_y[['Unnamed: 0', 'target']]\n",
    "test_y.to_csv('test_y.csv')\n",
    "test_y.head(8) "
   ]
  }
 ]
}