{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitadlvenvbd9a5c10f53f49eca40cc1477bddc462",
   "display_name": "Python 3.8.5 64-bit ('adl': venv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Imbalance classfication problem\n",
    "\n",
    "The jupyter notebook includes all related algorithms and results for loading dataset, visualizing principal conponents, and, predicting the data's label using Logistic regression and a designed neural network. The problem will be examined several mehtods and ways.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Imbalanced classfication problem (Rare event prediction, Extreme event prediction, Severe class imbalance) refers to a class distribution that is inherently not balanced. As we will see, the classes distribution are not balanced.\n",
    " \n",
    "There are perhaps two main groups of causes for the imbalance we may want to consider; they are data sampling and properties of the domain. Errors may have been made when collecting the observations. One type of error might have been applying the wrong class labels to many examples. Alternately, the processes or systems from which examples were collected may have been damaged or impaired to cause the imbalance.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "NUM_PCA_COMPONENT: int = 2\n",
    "PCA_VARIANCE: np.float = 0.99\n",
    "NUM_LDA_COMPONENT: int = 2\n",
    "\n",
    "\n",
    "def get_data() -> List[pd.DataFrame]:\n",
    "    input_train_data = pd.read_csv('train_x.csv')\n",
    "    output_train_data = pd.read_csv('train_y.csv')\n",
    "    input_test_data = pd.read_csv('test_x.csv')\n",
    "    return input_train_data, output_train_data, input_test_data\n",
    "\n",
    "\n",
    "def standardize_data(input_train_data: pd.DataFrame, input_test_data: pd.DataFrame) -> List[np.ndarray]:\n",
    "    in_train = StandardScaler().fit_transform(input_train_data)\n",
    "    in_test = StandardScaler().fit_transform(input_test_data)\n",
    "    return in_train, in_test\n",
    "\n",
    "\n",
    "def project_data(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    pca = PCA(n_components=NUM_PCA_COMPONENT)\n",
    "    principalComponents = pca.fit_transform(data)\n",
    "    principalDf = pd.DataFrame(data = principalComponents,\n",
    "                                columns = ['principal component 1', 'principal component 2'])\n",
    "    return principalDf, pca\n",
    "\n",
    "\n",
    "def visualize_data_pca(final_df: pd.DataFrame) -> plt.plot:\n",
    "    fig = plt.figure(figsize = (6,6))\n",
    "    ax = fig.add_subplot(1,1,1) \n",
    "    ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "    ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "    ax.set_title('2 component PCA', fontsize = 20)\n",
    "    targets = [1, 0]\n",
    "    colors = ['g', 'r']\n",
    "    for target, color in zip(targets,colors):\n",
    "        indicesToKeep = final_df['target'] == target\n",
    "        ax.scatter(final_df.loc[indicesToKeep, 'principal component 1'],\n",
    "                    final_df.loc[indicesToKeep, 'principal component 2'],\n",
    "                    c = color,\n",
    "                    s = 50)\n",
    "    ax.legend(targets)\n",
    "    ax.grid()\n",
    "\n",
    "\n",
    "def visualize_data_lda(input_train_data: pd.DataFrame, out_train_data: pd.DataFrame) -> plt.plot:\n",
    "    y = out_train_data.target\n",
    "    X = input_train_data\n",
    "    y.loc[4001] = y.loc[2] + 2\n",
    "    X.loc[4001] = input_train_data.loc[0] * 10\n",
    "\n",
    "    lda = LinearDiscriminantAnalysis(n_components=NUM_LDA_COMPONENT)\n",
    "    # X_r = lda.fit(X, y).transform(X)\n",
    "    X_lda = lda.fit_transform(X, y)\n",
    "    colors = ['navy', 'red']\n",
    "    target_names = ['False', 'True']\n",
    "    plt.figure()\n",
    "    for color, i, target_name in zip(colors, [0, 1], target_names):\n",
    "        plt.scatter(X_lda[y == i, 0], X_lda[y == i, 1], alpha=.8, color=color,\n",
    "                    label=target_name)\n",
    "    plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "    plt.title('2 LDA of data')\n",
    "    plt.xlabel('LDA 1')\n",
    "    plt.ylabel('LDA 2')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    print(\"\\n\")\n",
    "    print(f\"The LDA variance ratios are {lda.explained_variance_ratio_}\\n\")"
   ]
  },
  {
   "source": [
    "# Examining the possibility of exisiting imbalance in the dataset\n",
    "\n",
    "In this step, the train data and test data will be loaded, then, we should examine the number of data in each classes to evaluate the possibility of existing imbalance between classes. Besides, the histogram of data will be plotted to examine the issue.\n",
    "\n",
    "### Two types of Imbalance:\n",
    "\n",
    "Slight Imbalance. An imbalanced classification problem where the distribution of examples is uneven by a small amount in the training dataset (e.g. 4:6).\n",
    "\n",
    "Severe Imbalance. An imbalanced classification problem where the distribution of examples is uneven by a large amount in the training dataset (e.g. 1:100 or more).\n",
    "\n",
    "#### Problem:\n",
    "\n",
    "The minority class is harder to predict because there are few examples of this class, by definition."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train_data, out_train_data, input_test_data = get_data()\n",
    "data = input_train_data.copy()\n",
    "data['target'] = out_train_data['target']\n",
    "classes_ratio = (len(data[data['target'] == 1]) / len(data[data['target'] == 0]))\n",
    "if classes_ratio <= 0.6:\n",
    "    print(\"There exists severe imablance!!\\n\")\n",
    "if classes_ratio >= 0.6 and classes_ratio <= 0.95:\n",
    "    print(\"There exists slight imbalance!!\\n\")\n",
    "\n",
    "data.target.value_counts().plot(kind='bar', title='Count (target)', color = ['b', 'g'], grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = data.groupby(\"target\")\n",
    "for name, group in groups:\n",
    "    plt.plot(group[0], group[1], marker=\"o\", linestyle=\"\", label=name)\n",
    "plt.legend()"
   ]
  },
  {
   "source": [
    "As the figure above idicates, the frequency of classes is not balanced, and the first class (Target == zero) has much more samples than the other."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Visualize Principal Components\n",
    "\n",
    "The dataset should be normalized in order to avoid any problem not only in learning phase but also in computing principal components. Eventually, the data will be shown by considering principal components.\n",
    "\n",
    "Since PCA yields a feature subspace that maximizes the variance along the axes, it makes sense to standardize the data, especially, if it was measured on different scales. The algorithm standardize approach is the transformation of the data onto unit scale (mean=0 and variance=1), which is a requirement for the optimal performance of many machine learning algorithms. Moreover, Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual feature do not more or less look like standard normally distributed data.\n",
    "\n",
    "Here we plot the different samples on the 2 first principal components."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_train, in_test = standardize_data(input_train_data, input_test_data)\n",
    "principal_df, pca = project_data(in_train)\n",
    "final_df = pd.concat([principal_df, out_train_data[['target']]], axis = 1)\n",
    "visualize_data_pca(final_df)\n",
    "print(f\"The pca's variance ratio is {pca.explained_variance_ratio_}\\n\")"
   ]
  },
  {
   "source": [
    "The plot of the dataset is created showing the large mass of examples for the majority class (red) and a small number of examples for the minority class (green), with some class overlap."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Linear Discriminant Analysis (LDA)\n",
    "\n",
    "LDA tries to identify attributes that account for the most variance between classes. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels.\n",
    "\n",
    "Teh plot of the dataset is created showing the large mass of examples for the majority class (blue) and a small number of examples for the minority class (red), with some class overlap"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_data_lda(input_train_data, out_train_data)"
   ]
  },
  {
   "source": [
    "# Applying PCA to the Data\n",
    "Principal Component Analysis (PCA) applied to this data identifies the combination of attributes (principal components, or directions in the feature space) that account for the most variance in the data. As defined in the first cell, the pcas' variance is 0.98, so there are forty five principal components for this value. By increasing its value, the number of principal components will increase and vice-versa."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(PCA_VARIANCE)\n",
    "pca.fit(in_train)\n",
    "print(f\"Num of principal components is {pca.n_components_}\\n\")\n",
    "train_ = pca.transform(in_train)\n",
    "test_ = pca.transform(in_test)"
   ]
  },
  {
   "source": [
    "# Logistic Regression\n",
    "\n",
    "The fitting problem for train data will be computed by using Logistic Regression in this step. Then, the fitting accuracy will be computed for the seperated test data. Eventually, the target for the test data will be calculated using the obtained parameters in LogisticRegression. And, finally, the result is saved as a CSV file.\n",
    "\n",
    "* accuracy (fraction of correct predictions): correct predictions / total number of data points"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1, test_1, train_lbl, test_lbl = train_test_split(train_, out_train_data.target, test_size=1/5.0, random_state=0)\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "logisticRegr.fit(train_1, train_lbl)\n",
    "\n",
    "print(f\"The prediction score on test:    {str(logisticRegr.score(test_1, test_lbl))}\\n\")\n",
    "print(f\"The prediction score on train:   {str(logisticRegr.score(train_1, train_lbl))}\\n\")\n",
    "\n",
    "predicted_test = logisticRegr.predict(test_)\n",
    "test_y = pd.DataFrame(predicted_test, columns=['target'])\n",
    "test_y['Unnamed: 0'] = out_train_data['Unnamed: 0']\n",
    "test_y = test_y[['Unnamed: 0', 'target']]\n",
    "test_y.to_csv('test_y.csv')\n",
    "test_y.head(8) "
   ]
  },
  {
   "source": [
    "## NOTES on Logisitic Regression:\n",
    "\n",
    "1) The difference betweeen train score and test score is not significant, then, there does not exist any overfitting in the logistic regression fitting.\n",
    "\n",
    "2) As the results of prediction clarify, all of the predicted targets are zero, which indicate that the logistic regression is not able to solve the imbalance classification problem, and causes the following problem. \n",
    "\n",
    "* The abundance of samples from the majority class can swamp the minority class. Most machine learning algorithms for classification predictive models are designed and demonstrated on problems that assume an equal distribution of classes. This means that a naive application of a model may focus on learning the characteristics of the abundant observations only, neglecting the examples from the minority class that is, in fact, of more interest and whose predictions are more valuable.\n",
    "\n",
    "* The learning process of most classification algorithms is often biased toward the majority class examples, so that minority ones are not well modeled into the final system.\n",
    "\n",
    "Hence, we are going to use deep learning methods to solve the problem."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Neural Network (Deep Learning)\n",
    "\n",
    "Deep learning uses an artificial neural network that uses multiple layers to progressively extract higher level features from the training data. We are using a simple three-layer network without any optimisation, except the usage of a small validation dataset. \n",
    "\n",
    "\n",
    "# Regularisation\n",
    "\n",
    "In the following, I applied the 3 best practices for handling overfitting in a neural network:\n",
    "\n",
    "1) reduce the network’s size\n",
    "\n",
    "2) adding some weight regularisation\n",
    "\n",
    "3) adding dropout\n",
    "\n",
    "## Metrics\n",
    "\n",
    "There are many metrics for evaluating how good a binary classifier is doing in predicting the class labels for instances/examples. Below are some caveats and suggestions for choosing and interpreting the appropriate metrics.\n",
    "\n",
    "* Accuracy can be misleading. Since accuracy is simple the ratio of correctly predicted instances over all instances used for evaluation, it is possible to get a decent accuracy while having mostly incorrect predictions for the minority class.\n",
    "\n",
    "* Confusion matrix helps break down the predictive performances on different classes.\n",
    "\n",
    "Therefore, we utilize a combination of confusion matrix metrics and specifity-sensivity for solving the problem. Beside, we can compute the classes' weight, then use these weights for fitting problem in order to decrease imbalancing."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers, losses, metrics, regularizers, callbacks, initializers\n",
    "\n",
    "bias_initializer = initializers.HeNormal()\n",
    "\n",
    "NUM_VALIDATION: int = int(len(train_) * 0.2)\n",
    "\n",
    "train_1, test_1, train_lbl, test_lbl = train_test_split(train_, out_train_data.target, test_size=1/8.0, random_state=0)\n",
    "x_validation = train_1[:NUM_VALIDATION]\n",
    "x_partial_train = train_1[NUM_VALIDATION:]\n",
    "y_validation = train_lbl[:NUM_VALIDATION]\n",
    "y_partial_train = train_lbl[NUM_VALIDATION:]\n",
    "\n",
    "# weight_for_0: np.float = 1.0 / len(out_train_data[out_train_data['target'] ==0])\n",
    "# weight_for_1: np.float = 1.0 / len(out_train_data[out_train_data['target'] ==1])\n",
    "weight_for_0 = (1 / len(out_train_data[out_train_data['target'] ==0])) * (len(out_train_data)) / 2.0\n",
    "weight_for_1 = (1 / len(out_train_data[out_train_data['target'] ==1])) * (len(out_train_data)) / 2.0\n",
    "class_weights = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "metrics_ = [\n",
    "    metrics.FalseNegatives(name=\"fn\"),\n",
    "    metrics.FalsePositives(name=\"fp\"),\n",
    "    metrics.TrueNegatives(name=\"tn\"),\n",
    "    metrics.TruePositives(name=\"tp\"),\n",
    "    metrics.Precision(name=\"precision\"),\n",
    "    metrics.Recall(name=\"recall\"),\n",
    "    metrics.AUC(name='auc'),\n",
    "    metrics.AUC(name='prc', curve='PR')\n",
    "]\n",
    "\n",
    "\n",
    "def dl_model() -> models.Model:\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(16, activation=\"relu\", input_shape=(train_1.shape[-1],)))\n",
    "    # model.add(layers.Dense(16, activation='relu'))\n",
    "    model.add(layers.Dense(16, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid', bias_initializer=bias_initializer))\n",
    "    model.compile(optimizer=optimizers.Adam(lr=1e-3),\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=metrics_)\n",
    "    return model\n",
    "\n",
    "\n",
    "def regularized_dl_model() -> models.Model:\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation=\"relu\", input_shape=(train_1.shape[-1],)))\n",
    "    model.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.003), activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.003), activation='relu'))\n",
    "    model.add(layers.Dropout(0.6))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=metrics_)\n",
    "    return model\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_prc', \n",
    "                                        verbose=1,\n",
    "                                        patience=10,\n",
    "                                        mode='max',\n",
    "                                        restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "model = dl_model()\n",
    "model.fit(x_partial_train, y_partial_train,\n",
    "        epochs=100,\n",
    "        batch_size=16,\n",
    "        verbose=2,\n",
    "        shuffle=True,\n",
    "        validation_data=(x_validation, y_validation),\n",
    "        class_weight=class_weights)\n",
    "\n",
    "print(\"\\n\")\n",
    "model.summary()\n",
    "print(\"\\n\")\n",
    "print(f\"score on test:  {str(model.evaluate(test_1, test_lbl)[1])}\\n\")\n",
    "print(f\"score on train: {str(model.evaluate(train_1, train_lbl)[1])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "model_regularized = regularized_dl_model()\n",
    "model_regularized.fit(x_partial_train, y_partial_train,\n",
    "        epochs=300,\n",
    "        batch_size=64,\n",
    "        verbose=2,\n",
    "        validation_data=(x_validation, y_validation),\n",
    "        class_weight=class_weights)\n",
    "\n",
    "print(\"\\n\")\n",
    "model_regularized.summary()\n",
    "print(\"\\n\")\n",
    "print(f\"score on test:  {str(model_regularized.evaluate(test_1, test_lbl)[1])}\\n\")\n",
    "print(f\"score on train:  {str(model_regularized.evaluate(train_1, train_lbl)[1])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(test_)  # np.argmax(model.predict(x), axis=-1)\n",
    "test_y = pd.DataFrame(predictions, columns=['target'])\n",
    "test_y['Unnamed: 0'] = out_train_data['Unnamed: 0']\n",
    "test_y = test_y[['Unnamed: 0', 'target']]\n",
    "test_y.to_csv('test_y.csv')\n",
    "test_y.head(8) \n",
    "sum(test_y['target'])"
   ]
  },
  {
   "source": [
    "## Notes on Neural Nework's result\n",
    "\n",
    "* As the test score of the first model is higher than the regularized model, we use this model to predict targets of the test dataset.\n",
    "\n",
    "* The regularized model has much less train/total parameter, then it's architecture has benefits rather than the firs one.\n",
    "\n",
    "* According to the result, using the mentioned metrics cause to overcome to imbalance problem."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Data Manipulation\n",
    "\n",
    "There exists several ways to balance the number of smaples in each classes. As mentioned before, there exists severe imbalance between the classes. As shown above, Most machine learning techniques will ignore the minority class, and thus, perform poorly on the observations we are most interested in. Typical challenges are biased predictions and appropriate selection of score type. There are multiple approaches to address data imbalance and the approaches could primarily focus on random over/undersampling and SMOTE (Synthetic Minority Oversampling Technique) for tabular data. I am going to explain and use SMOTE method for augmenting data.\n",
    "\n",
    "\n",
    "\n",
    "## Upsampling using SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "A data augmentation technique for tabular data. SMOTE is available in Python using the imblearn library. SMOTE creates new data points based on the existing minority class data points using linear combinations of feature vectors. \n",
    "\n",
    "In the folllowing steps, the dataset will be augmented, then, the previous approaches/algorithms will be applied to examine the results.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "input_train_data, out_train_data, input_test_data = get_data()\n",
    "\n",
    "over = SMOTE(sampling_strategy=1)\n",
    "data_train, out = over.fit_resample(input_train_data, out_train_data['target'])\n",
    "data_target = pd.DataFrame(out, columns=['target'])\n",
    "data_target['Unnamed: 0'] = data_train['Unnamed: 0']\n",
    "data_target = data_target[['Unnamed: 0', 'target']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_train.copy()\n",
    "data['target'] = data_target['target']\n",
    "classes_ratio = (len(data[data['target'] == 1]) / len(data[data['target'] == 0]))\n",
    "if classes_ratio <= 0.6:\n",
    "    print(\"There exists severe imablance!!\")\n",
    "if classes_ratio >= 0.6 and classes_ratio <= 0.95:\n",
    "    print(\"There exists slight imbalance!!\")\n",
    "else:\n",
    "    print(\"Dataset balancing looks good!!!\")\n",
    "\n",
    "data['target'].plot.hist(bins=2, alpha=0.96, color=['#A0E8AF', '#FFCF56'], rwidth=0.99, grid=True, histtype='bar', title='Histogram Of classes frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_train, in_test = standardize_data(data_train, input_test_data)\n",
    "principal_df, pca = project_data(in_train)\n",
    "final_df = pd.concat([principal_df, data_target['target']], axis=1)\n",
    "visualize_data_pca(final_df)\n",
    "print(f\"The pca's variance ratio is {pca.explained_variance_ratio_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_data_lda(data_train, data_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(PCA_VARIANCE)\n",
    "pca.fit(in_train)\n",
    "print(f\"Num of principal components is {pca.n_components_}\")\n",
    "train_ = pca.transform(in_train)\n",
    "test_ = pca.transform(in_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1, test_1, train_lbl, test_lbl = train_test_split(train_, data_target.target, test_size=1/7.0, random_state=0)\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "logisticRegr.fit(train_1, train_lbl)\n",
    "print(\"The prediction score on test: \" + str(logisticRegr.score(test_1, test_lbl)))\n",
    "print(\"The prediction score on train: \"+ str(logisticRegr.score(train_1, train_lbl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers, losses, metrics, regularizers\n",
    "\n",
    "\n",
    "NUM_VALIDATION: int = int(len(train_) * 0.2)\n",
    "\n",
    "train_1, test_1, train_lbl, test_lbl = train_test_split(train_, data_target.target, test_size=1/5.0, random_state=0)\n",
    "x_validation = train_1[:NUM_VALIDATION]\n",
    "x_partial_train = train_1[NUM_VALIDATION:]\n",
    "y_validation = train_lbl[:NUM_VALIDATION]\n",
    "y_partial_train = train_lbl[NUM_VALIDATION:]\n",
    "\n",
    "metrics_ = [\n",
    "    metrics.FalseNegatives(name=\"fn\"),\n",
    "    metrics.FalsePositives(name=\"fp\"),\n",
    "    metrics.TrueNegatives(name=\"tn\"),\n",
    "    metrics.TruePositives(name=\"tp\"),\n",
    "    metrics.Precision(name=\"precision\"),\n",
    "    metrics.Recall(name=\"recall\"),\n",
    "    metrics.AUC(name='auc'),\n",
    "    metrics.AUC(name='prc', curve='PR')\n",
    "]\n",
    "\n",
    "\n",
    "def dl_model() -> models.Model:\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(16, activation=\"relu\", input_shape=(train_1.shape[-1],)))\n",
    "    # model.add(layers.Dense(16, activation='relu'))\n",
    "    model.add(layers.Dense(16, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid', bias_initializer=bias_initializer))\n",
    "    model.compile(optimizer=optimizers.Adam(lr=1e-3),\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=metrics_)\n",
    "    return model\n",
    "\n",
    "\n",
    "def regularized_dl_model() -> models.Model:\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(16, activation=\"relu\", input_shape=(train_1.shape[-1],)))\n",
    "    model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.003), activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.003), activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=metrics_)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "model = dl_model()\n",
    "model.fit(x_partial_train, y_partial_train,\n",
    "        epochs=500,\n",
    "        batch_size=32,\n",
    "        verbose=2,\n",
    "        validation_data=(x_validation, y_validation),\n",
    "        shuffle=True)\n",
    "\n",
    "print(\"\\n\")\n",
    "model.summary()\n",
    "print(\"\\n\")\n",
    "print(f\"score on test:  {str(model.evaluate(test_1, test_lbl)[1])}\\n\")\n",
    "print(f\"score on train: {str(model.evaluate(train_1, train_lbl)[1])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "model_regularized = regularized_dl_model()\n",
    "model_regularized.fit(x_partial_train, y_partial_train,\n",
    "        epochs=300,\n",
    "        batch_size=64,\n",
    "        verbose=2,\n",
    "        validation_data=(x_validation, y_validation))\n",
    "\n",
    "print(\"\\n\")\n",
    "model_regularized.summary()\n",
    "print(\"\\n\")\n",
    "print(f\"score on test:  {str(model_regularized.evaluate(test_1, test_lbl)[1])}\\n\")\n",
    "print(f\"score on train:  {str(model_regularized.evaluate(train_1, train_lbl)[1])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(test_)\n",
    "test_y = pd.DataFrame(predictions, columns=['target'])\n",
    "test_y['Unnamed: 0'] = out_train_data['Unnamed: 0']\n",
    "test_y = test_y[['Unnamed: 0', 'target']]\n",
    "test_y.to_csv('test_y.csv')\n",
    "test_y.head(8) \n",
    "sum(test_y['target'])"
   ]
  }
 ]
}